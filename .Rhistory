rename("Number of Manifestos" = n, "Year" = date) %>%
print()
UK1980_tidied_corpus_ed <- UK1980_tidied_corpus %>% mutate(id=str_replace_all(paste0(party," ",date)," "," "))
UK1980_tidied_corpus_ed <- UK1980_tidied_corpus_ed %>% select(id, text)
head(UK1980_tidied_corpus_ed)
UK1980_dfm <- UK1980_tidied_corpus_ed %>%
data.frame(with.meta = TRUE) %>%
corpus(docid_field = "id", unique_docnames = FALSE) %>%
tokens(remove_punct = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
dfm() %>%
dfm_trim(min_termfreq = 5)
# Creating a tf_idf
UK1980_tf_idf <- UK1980_dfm %>%
tidy() %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
View(UK1980_tf_idf)
# Assuming UK1980_dfm is your Document Feature Matrix
UK1980_tf_idf <- UK1980_dfm %>%
dfm_remove(stopwords("english")) %>%
tidy() %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
# Checking how often migration comes up as a topic
UK1980_td <- tidy(UK1980_dfm)
UK1980_term_change <- UK1980_td %>%
extract(document, "year", "_([0-9]{4})", convert = TRUE) %>%
complete(year, term, fill = list(count = 0)) %>%
group_by(year) %>%
mutate(year_total = sum(count))
UK1980_term_change %>%
filter(term %in% c("immigration", "immigrant", "immigrants", "migration", "refugees")) %>%
ggplot(aes(year, count / year_total)) +
geom_point() +
geom_smooth() +
facet_wrap(~ term, scales = "free_y") +
scale_y_continuous(labels = scales::percent_format()) +
ylab("% frequency of word in party manifesto")
# see rough code
UK1980_lda <- LDA(UK1980_dfm, k = 6)
#control = list(seed = 1234)
W <- UK1980_lda@gamma
H <- UK1980_lda@beta
topic_words_top <- tidy(UK1980_lda, matrix="beta")
topic_words_top
topic_words_top <- tidy(UK1980_lda, matrix="beta") %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
topic_words_top %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# I think that if you don't separate out the manifestos from one another you end up treating everything the same, which is no good.
# Checking how often migration comes up as a topic
UK1980_td <- tidy(UK1980_dfm)
UK1980_term_change <- UK1980_td %>%
extract(document, "year", "_([0-9]{4})", convert = TRUE) %>%
complete(year, term, fill = list(count = 0)) %>%
group_by(year) %>%
mutate(year_total = sum(count))
UK1980_term_change %>%
filter(term %in% c("immigration", "immigrant", "immigrants", "migration", "refugees")) %>%
ggplot(aes(year, count / year_total)) +
geom_point() +
geom_smooth() +
facet_wrap(~ .term, scales = "free_y") +  # Corrected here
scale_y_continuous(labels = scales::percent_format()) +
ylab("% frequency of word in party manifesto")
# Using manifestoR package for dataset
library(manifestoR)
# Setting API key
mp_setapikey("manifesto_apikey.txt")
# Checking availability of United Kingdom data set for good practice
available_docs <- mp_availability(countryname == "United Kingdom")
available_docs
# Downloading a corpus of British manifestos between 1980 and 2020
UK1980_corpus <- mp_corpus(countryname == "United Kingdom" & date > 198000 & date < 202000 & partyname %in%
c("Conservative Party",
"Labour Party",
"Liberal Democrats",
"Liberal Party",
"Social Democratic Party"))
# Tidying corpus using tidytext function tidy
UK1980_tidied_corpus <- UK1980_corpus %>%
tidy()
UK1980_tidied_corpus <- UK1980_tidied_corpus %>% mutate(party=case_when(party==51320~"Labour Party",
party==51620~"Conservative Party",
party==51421~"Liberal Democrats",
party==51330~"Social Democratic Party",
party==51420~"Liberal Party"),
date=substr(date,1,4))
#drop columns that don't give important info
UK1980_tidied_corpus <- UK1980_tidied_corpus %>% select(party, date, text)
head(UK1980_tidied_corpus)
# checking corpus for years and number of texts
checking_corpus_summary <- UK1980_tidied_corpus %>%
count(date) %>%
bind_rows(summarize(., n = sum(n))) %>%
rename("Number of Manifestos" = n, "Year" = date) %>%
print()
UK1980_tidied_corpus_ed <- UK1980_tidied_corpus %>% mutate(id=str_replace_all(paste0(party," ",date)," "," "))
UK1980_tidied_corpus_ed <- UK1980_tidied_corpus_ed %>% select(id, text)
head(UK1980_tidied_corpus_ed)
UK1980_dfm <- UK1980_tidied_corpus_ed %>%
data.frame(with.meta = TRUE) %>%
corpus(docid_field = "id", unique_docnames = FALSE) %>%
tokens(remove_punct = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
dfm() %>%
dfm_trim(min_termfreq = 5)
View(UK1980_tidied_corpus_ed)
# Assuming UK1980_dfm is your Document Feature Matrix
UK1980_tf_idf <- UK1980_dfm %>%
dfm_remove(stopwords("english")) %>%
tidy() %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
UK1980_term_change <- UK1980_tf_idf %>%
#  extract(document, "year", "_([0-9]{4})", convert = TRUE) %>%
complete(year, term, fill = list(count = 0)) %>%
group_by(year) %>%
mutate(year_total = sum(count))
View(UK1980_tf_idf)
UK1980_td <- tidy(UK1980_dfm)
View(UK1980_td)
UK1980_term_change <- UK1980_tf_idf %>%
extract(document, "year", "(\\d+)", convert = TRUE) %>%
complete(year, term, fill = list(count = 0)) %>%
group_by(year) %>%
mutate(year_total = sum(count))
UK1980_term_change %>%
filter(term %in% c("immigration", "immigrant", "immigrants", "migration", "refugees")) %>%
ggplot(aes(year, count / year_total)) +
geom_point() +
geom_smooth() +
facet_wrap(~ term, scales = "free_y") +
scale_y_continuous(labels = scales::percent_format()) +
ylab("% frequency of word in party manifesto")
View(UK1980_tf_idf)
# Assuming UK1980_dfm is your Document Feature Matrix
UK1980_tf_idf <- UK1980_dfm %>%
dfm_remove(stopwords("english")) %>%
tidy() %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
View(UK1980_tf_idf)
# Assuming UK1980_dfm is your Document Feature Matrix
UK1980_tf_idf <- UK1980_dfm %>%
dfm_remove(stopwords("en")) %>%
tidy() %>%
bind_tf_idf(term, document, count) %>%
arrange(desc(tf_idf))
View(UK1980_tf_idf)
# Checking how often migration comes up as a topic
UK1980_td <- tidy(UK1980_dfm)
UK1980_term_change <- UK1980_tf_idf %>%
extract(document, "year", "(\\d+)", convert = TRUE) %>%
complete(year, term, fill = list(count = 0)) %>%
group_by(year) %>%
mutate(year_total = sum(count))
UK1980_term_change %>%
filter(term %in% c("immigration", "immigrant", "immigrants", "migration", "refugees")) %>%
ggplot(aes(year, count / year_total)) +
geom_point() +
geom_smooth() +
facet_wrap(~ term, scales = "free_y") +
scale_y_continuous(labels = scales::percent_format()) +
ylab("% frequency of word in party manifesto")
# see rough code
UK1980_lda <- LDA(UK1980_dfm, k = 6)
#control = list(seed = 1234)
W <- UK1980_lda@gamma
H <- UK1980_lda@beta
topic_words_top <- tidy(UK1980_lda, matrix="beta")
topic_words_top
topic_words_top <- tidy(UK1980_lda, matrix="beta") %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
topic_words_top %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
# I think that if you don't separate out the manifestos from one another you end up treating everything the same, which is no good.
View(UK1980_tidied_corpus_ed)
View(UK1980_tidied_corpus_ed)
for (i in 1:29){
name = UK1980_tidied_corpus_ed$id[i]
sentences = unlist(strsplit(UK1980_tidied_corpus_ed$text[i],split = "\n"))
ids = rep(UK1980_tidied_corpus_ed$id[i], sapply(strsplit(UK1980_tidied_corpus_ed$text[i], split = "\n"), length))
new_ids = paste(ids, sequence(sapply(strsplit(UK1980_tidied_corpus_ed$text[i], split = "\n"), length)), sep = "_")
db_list[[name]] = data.frame(id = new_ids, text = sentences)
rownames(db_list[[name]]) = NULL
}
db_list <- list()
for (i in 1:29){
name = UK1980_tidied_corpus_ed$id[i]
sentences = unlist(strsplit(UK1980_tidied_corpus_ed$text[i],split = "\n"))
ids = rep(UK1980_tidied_corpus_ed$id[i], sapply(strsplit(UK1980_tidied_corpus_ed$text[i], split = "\n"), length))
new_ids = paste(ids, sequence(sapply(strsplit(UK1980_tidied_corpus_ed$text[i], split = "\n"), length)), sep = "_")
db_list[[name]] = data.frame(id = new_ids, text = sentences)
rownames(db_list[[name]]) = NULL
}
View(db_list)
library(tm)
library(SnowballC)
preprocess_text <- function(text) {
text <- tolower(text)  # Convert to lowercase
text <- gsub("[[:punct:]]", "", text)  # Remove punctuation
text <- gsub("[[:digit:]]", "", text)  # Remove numbers
text <- gsub("[ \t\r\n]+", " ", text)  # Remove extra whitespaces
text <- trimws(text)  # Remove leading and trailing whitespaces
text <- removeWords(text, stopwords("spanish"))  # Remove stopwords
text <- wordStem(text, language = "spanish")  # Stemming
}
namedb <- names(db_list)
for (i in 1:6){
name = namedb[i]
db_list[[name]][2] = sapply(db_list[[name]][2], preprocess_text)
}
View(UK1980_tidied_corpus_ed)
View(db_list)
library(quanteda)
dfmat_list<-list()
for (i in 1:6){
name = namedb[i]
dfmat_list[[name]] = unlist(db_list[[name]][2]) %>%
tokens() %>%
dfm() #%>%
#dfm_trim(min_termfreq = 5)
}
View(db_list)
library(topicmodels)
lda_list <- list()
SEED = set.seed(1500)
#this takes like 3 minutes
for (i in 1:29){
name = namedb[i]
lda_list[[name]] = LDA(dfmat_list[[name]], 10, control = list(alpha=5),seed=SEED)
}
db_list <- list()
for (i in 1:29){
name = UK1980_tidied_corpus_ed$id[i]
sentences = unlist(strsplit(UK1980_tidied_corpus_ed$text[i],split = "\n"))
ids = rep(UK1980_tidied_corpus_ed$id[i], sapply(strsplit(UK1980_tidied_corpus_ed$text[i], split = "\n"), length))
new_ids = paste(ids, sequence(sapply(strsplit(UK1980_tidied_corpus_ed$text[i], split = "\n"), length)), sep = "_")
db_list[[name]] = data.frame(id = new_ids, text = sentences)
rownames(db_list[[name]]) = NULL
}
library(tm)
library(SnowballC)
preprocess_text <- function(text) {
text <- tolower(text)  # Convert to lowercase
text <- gsub("[[:punct:]]", "", text)  # Remove punctuation
text <- gsub("[[:digit:]]", "", text)  # Remove numbers
text <- gsub("[ \t\r\n]+", " ", text)  # Remove extra whitespaces
text <- trimws(text)  # Remove leading and trailing whitespaces
text <- removeWords(text, stopwords("spanish"))  # Remove stopwords
text <- wordStem(text, language = "spanish")  # Stemming
return(text)  # Add this line
}
namedb <- names(db_list)
for (i in 1:29) {  # Change to 29
name = namedb[i]
db_list[[name]][2] <- sapply(db_list[[name]][2], function(text) if (!is.null(text)) preprocess_text(text))
}
library(quanteda)
dfmat_list<-list()
for (i in 1:29){
name = namedb[i]
dfmat_list[[name]] = unlist(db_list[[name]][2]) %>%
tokens() %>%
dfm() #%>%
#dfm_trim(min_termfreq = 5)
}
library(topicmodels)
lda_list <- list()
SEED = set.seed(1500)
for (i in 1:29) {  # Change to 29
name = namedb[i]
if (!is.empty.dfmat(dfmat_list[[name]])) {  # Check if dfmat is not empty
lda_list[[name]] = LDA(dfmat_list[[name]], 10, control = list(alpha=5), seed = SEED)
}
}
library(topicmodels)
lda_list <- list()
SEED <- set.seed(1500)
for (i in 1:29) {  # Change to 29
name <- namedb[i]
if (nfeature(dfmat_list[[name]]) > 0) {  # Check if dfmat is not empty
lda_list[[name]] <- LDA(dfmat_list[[name]], 10, control = list(alpha = 5), seed = SEED)
}
}
library(topicmodels)
lda_list <- list()
SEED = set.seed(1500)
for (i in 1:29) {  # Change to 29
name <- namedb[i]
if (ndoc(dfmat_list[[name]]) > 0) {  # Check if dfmat is not empty
lda_list[[name]] <- LDA(dfmat_list[[name]], 10, control = list(alpha = 5), seed = SEED)
}
}
library(topicmodels)
lda_list <- list()
SEED = set.seed(1500)
for (i in 1:29) {  # Change to 29
name <- namedb[i]
if (ndoc(dfmat_list[[name]]) > 0 && any(row_sums(dfmat_list[[name]]) > 0)) {
lda_list[[name]] <- LDA(dfmat_list[[name]], 10, control = list(alpha = 5), seed = SEED)
}
}
library(topicmodels)
lda_list <- list()
SEED = set.seed(1500)
for (i in 1:29) {  # Change to 29
name <- namedb[i]
if (ndoc(dfmat_list[[name]]) > 0 && any(rowSums(dfmat_list[[name]]) > 0)) {
lda_list[[name]] <- LDA(dfmat_list[[name]], 10, control = list(alpha = 5), seed = SEED)
}
}
library(topicmodels)
lda_list <- list()
SEED = set.seed(1500)
for (i in 1:29) {  # Change to 29
name <- namedb[i]
# Check if dfmat is not empty and contains at least one non-zero entry
if (ndoc(dfmat_list[[name]]) > 0 && any(apply(dfmat_list[[name]], 1, any))) {
lda_list[[name]] <- LDA(dfmat_list[[name]], 10, control = list(alpha = 5), seed = SEED)
}
}
library(topicmodels)
lda_list <- list()
SEED = set.seed(1500)
#this takes like 3 minutes
for (i in 1:29){
name = namedb[i]
lda_list[[name]] = LDA(dfmat_list[[name]], 10, control = list(alpha=5),seed=SEED)
}
View(lda_list)
knitr::opts_chunk$set(echo = TRUE)
# Using manifestoR package for dataset
library(manifestoR)
# Setting API key
mp_setapikey("manifesto_apikey.txt")
# Checking availability of United Kingdom data set for good practice
available_docs <- mp_availability(countryname == "United Kingdom")
available_docs
install.packages("prereg")
tinytex::install_tinytex()
install.packages("jsonlite")
install.packages("ggplot2")  # or any other visualization package you prefer
library(jsonlite)
json_data <- fromJSON("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran-lang-repos-postman.json")
View(json_data)
# This step heavily depends on the structure of your JSON file.
df <- as.data.frame(json_data)
View(df)
# data processing
library(tidyverse)
df %>%
select(id, name, full_name, html_url, description)
View(df)
df %>%
select(id, name, full_name, html_url, description)
selected_df <- df %>%
select(id, name, full_name, html_url, description)
# View the resulting data frame
print(selected_df)
View(selected_df)
# presenting the DF
install.packages("gridExtra")
install.packages("ggplot2")
library(gridExtra)
library(ggplot2)
table_plot <- tableGrob(selected_df)
png("selected_df.png", width = 800, height = 600)  # Adjust the size as needed
grid.draw(table_plot)
install.packages("ggplot2")
View(table_plot)
# By default, the code chunks are hidden for brevity. Set the echo argument to TRUE for blocks you want to share the code in the output for.
knitr::opts_chunk$set(echo = F)
knitr::include_graphics("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/pap/figures/Fortran FPM Pushes and Commits Over Time.png")
knitr::include_graphics("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/pap/figures/Fortran FPM Pushes and Commits Over Time.png")
knitr::include_graphics("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/pap/figures/Fortran FPM Pushes and Commits Over Time.png")
View(selected_df)
fortran-lang-repos-postman <- head(selected_df)
fortran_lang_repos_postman <- head(selected_df)
View(fortran_lang_repos_postman)
install.packages("jsonlite")
install.packages("ggplot2")  # or any other visualization package you prefer
library(jsonlite)
json_data <- fromJSON("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran-lang-repos-postman.json")
# This step heavily depends on the structure of your JSON file.
df <- as.data.frame(json_data)
# data processing
library(tidyverse)
selected_df <- df %>%
select(id, name, full_name, html_url, description)
fortran_lang_repos_postman <- head(selected_df)
knitr::kable(fortran_lang_repos_postman)
# Load packages and data to be clear from the start
library(jsonlite)
library(tidyverse)
json_data <- fromJSON("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran-lang-repos-postman.json")
df <- as.data.frame(json_data)
selected_df <- df %>%
select(id, name, full_name, html_url, description)
fortran_lang_repos_postman <- head(selected_df)
knitr::kable(fortran_lang_repos_postman)
# Load packages and data to be clear from the start
library(jsonlite)
library(tidyverse)
json_data <- fromJSON("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran-lang-repos-postman.json")
df <- as.data.frame(json_data)
selected_df <- df %>%
select(id, name, full_name, html_url, description)
fortran_lang_repos_postman <- head(selected_df)
# Load packages and data to be clear from the start
library(jsonlite)
library(tidyverse)
json_data <- fromJSON("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran-lang-repos-postman.json")
df <- as.data.frame(json_data)
selected_df <- df %>%
select(id, name, full_name, html_url, description)
fortran_lang_repos_postman <- head(selected_df)
knitr::kable(fortran_lang_repos_postman)
# Load packages and data to be clear from the start
library(jsonlite)
library(tidyverse)
json_data <- fromJSON("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran-lang-repos-postman.json")
df <- as.data.frame(json_data)
selected_df <- df %>%
select(id, full_name, html_url, description)
fortran_lang_repos_postman <- head(selected_df)
knitr::kable(fortran_lang_repos_postman)
fortran_lang_repos_postman <- head(selected_df)
knitr::kable(fortran_lang_repos_postman)
# By default, the code chunks are hidden for brevity. Set the echo argument to TRUE for blocks you want to share the code in the output for.
knitr::opts_chunk$set(echo = F)
library(readxl)
dataset_1 <- read_excel("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/investment dataset.xlsx")
head(dataset_1)
library(readxl)
library(knitr)
# Load the dataset
dataset_1 <- read_excel("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/investment dataset.xlsx")
# Display the first few rows with a more formatted table
kable(head(dataset_1), caption = "Head of Dataset 1")
fortran_lang_repos_postman <- head(selected_df)
# Load packages and data to be clear from the start
library(jsonlite)
library(tidyverse)
json_data <- fromJSON("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran-lang-repos-postman.json")
df <- as.data.frame(json_data)
selected_df <- df %>%
select(id, full_name, html_url, description)
View(selected_df)
View(df)
fortran_lang_repos_postman <- head(selected_df)
knitr::kable(fortran_lang_repos_postman)
kable(head(selected_df), caption = "Example of Repo Data From GitHub API")
# Load packages and data to be clear from the start
library(jsonlite)
library(tidyverse)
library(readxl)
library(knitr)
json_data <- fromJSON("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran-lang-repos-postman.json")
df <- as.data.frame(json_data)
selected_df <- df %>%
select(id, full_name, html_url, description)
dataset_1 <- read_excel("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/investment dataset.xlsx")
kable(head(dataset_1), caption = "Head of Dataset 1")
kable(head(selected_df), caption = "Example of Repo Data From GitHub API")
dataset_2_fortran <- read_excel("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran_fpm_pushes_commits_raw.xlsx")
# Display the first few rows with a more formatted table
kable(head(dataset_2_fortran), caption = "Example of Repo Activity Data From GitHub API")
dataset_2_fortran <- read_excel("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran_fpm_pushes_commits_raw.xlsx")
# Display the first few rows with a more formatted table
kable(head(dataset_2_fortran), caption = "Example of Repo Activity Data From GitHub API")
dataset_2_fortran <- read_excel("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran_fpm_pushes_commits_raw.xlsx")
# Display the first few rows with a more formatted table
kable(head(dataset_2_fortran), caption = "Example of Repo Activity Data From GitHub API")
dataset_2_fortran <- read_excel("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran_fpm_pushes_commits_raw.xlsx")
# Display the first few rows with a more formatted table
kable(head(dataset_2_fortran), caption = "Example of Repo Activity Data From GitHub API")
kable(head(selected_df), caption = "Example of Organisation Data From GitHub API - Repositories")
dataset_2_fortran <- read_excel("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/data/raw/fortran_test/fortran_fpm_pushes_commits_raw.xlsx")
# Display the first few rows with a more formatted table
kable(head(dataset_2_fortran), caption = "Example of Fortran FPM Repo Activity Data From GitHub API")
knitr::include_graphics("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/pap/figures/Fortran FPM Pushes and Commits Over Time.png")
knitr::include_graphics("Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/pap/figures/data_schema.png")
knitr::include_graphics("Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/pap/figures/data_schema.png")
knitr::include_graphics("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/pap/figures/data_schema.png")
knitr::include_graphics("/Users/paulsharratt/Documents/Hertie/Semester 3/06 - Master's Thesis/thesis_stf/pap/figures/data_schema.png")
# Set working directory and base path
base_path <- "/Users/paulsharratt/Documents/Hertie/Semester 4/03 - Master's Thesis/thesis_stf/"
setwd(base_path)
# Load necessary libraries
library(tidyverse)
library(lubridate)
repo_commits_alls <- read.csv(paste0(base_path, "data/raw/augur/repo_commits_all.csv"))
View(repo_commits_alls)
